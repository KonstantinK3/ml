{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4FFW+PvC312ydlTRhTSQhYYckIAiyCMoAAm4sYRGc\nEVHnJyOOiKJXuQgIiMtc3BC8o6OOCsroKHhBZRGcqCyBBBJICAFCwpKFrL0kvdT5/RFoE0kISrqr\nq/N+nscnXUtXf49F5U2dqj6lEkIIEBERkWKo5S6AiIiIfhuGNxERkcIwvImIiBSG4U1ERKQwDG8i\nIiKFYXgTEREpDMObSAbp6emYNWsWJk6ciAkTJuCBBx5Abm4uAODIkSN49NFH3V7Djh07sHz58kaX\nTZgwAXv37r1i/rFjx3Dbbbfh7rvvRmFhYYvW88Ybb2D79u0AgDVr1uDf//53i26fyJdo5S6AqLWx\n2Wx46KGH8O6776JXr14AgC+//BJz587Fjh070KdPH7z22mtur+PWW2/Frbfe+pves2PHDgwaNAgv\nvPBCi9ezd+9edO3aFQAwf/78Ft8+kS9heBN5mNVqRXV1NSwWi2veHXfcAYPBAKfTiQMHDmDZsmXY\nsmULysrK8PTTT+PMmTMICwuD0WhEfHw8/vKXv6BPnz744x//iO+//x4mkwkLFy7Etm3bcPz4cbRt\n2xZvv/02AgMDceDAAaxevRpWqxU6nQ6PPfYYhg8fjs8//xzffPMN1q1bhxMnTuCZZ56B1WpFbGxs\ng9ou++qrr/DJJ5/A6XSipqYGN998s+v9ABpsb9GiRTAYDMjJycGFCxcQGxuLV199FUFBQcjIyMDy\n5ctd9Tz55JM4efIkMjMzsXr1amg0GuzYsQPx8fGYM2fOVev/7rvvoFarkZ+fD51OhxdffBEJCQke\n25dEcmF4E3lYaGgoFi5ciAceeACRkZFITk7GoEGDMH78eOj1+gbrLl++HF27dsW6detQXFyMe+65\nB/Hx8QDqzuCNRiM2b96M9evX49lnn8XWrVthNBoxefJk7NixA0OHDsWjjz6KtWvXol+/fsjNzcW9\n996LTZs2NficJ554AjNnzsSUKVOQlpaGmTNnXlH3HXfcgfz8fJSXl2Px4sX4/PPPr9rOzMxMfPDB\nB1CpVJg6dSq2bduGO+64A4888giWL1+OW265BZmZmXj66afx5ZdfYtu2bZg5cyZGjx6NHTt2AADK\ny8uvWv/+/fuxZcsWtGvXDsuWLcPf//53vPjii7973xApBa95E8ngT3/6E1JTU/Hss8/CaDTinXfe\nwV133YXq6uoG6+3evRspKSkAgLZt22Ls2LENlo8ZMwYAEB0djYSEBERFRUGtVqNTp06orKzE4cOH\nER0djX79+gEA4uPjkZycjH379rm2UV5ejpycHNx1110AgP79+7v+QLgew4YNg16vh06nQ0JCAior\nK3H8+HGo1WrccsstAIDevXtj8+bNUKsb/1XUXP29evVCu3btAAA9e/ZEZWXldddNpAQMbyIPS0tL\nw//+7//CYDBg5MiRePLJJ/H1119DrVYjNTW1wbparRb1Hz/w65DT6XSNvr5MkqQr5gkh4HA4XNMq\nlco1v/7nNkelUjV4j91ub7Dc39//inU1Go3r8y47fvx4g3p+S/2NfQZRa8DwJvKwiIgIrF27FgcO\nHHDNKykpgdVqveJ67YgRI1xdxOXl5di+ffsV4Xc1/fr1w6lTp3D48GEAQG5uLvbv34+BAwe61gkL\nC0OvXr3w2WefAQCysrJw/Pjxa2pHbm4uamtr4XA4sGvXrmbfExsbC5VK5fojJSsrC/fddx8kSYJG\no7kixK+lfqLWiNe8iTysS5cuePPNN/G3v/0NFy5cgJ+fH4KDg7F06VLExsaipKTEte7TTz+NZ599\nFhMnTkRYWBg6dOjQ4GyzOREREVizZg2WLVuGmpoaqFQqrFy5El26dMGhQ4dc67366qt4+umnsWHD\nBkRHRyM2NrbZbd9888248cYbMW7cOBiNRgwaNAg5OTlXfY9er8frr7+OFStWYPXq1dDpdHj99deh\n1+sxcuRIvPjiiw3O4K+1fqLWRsVHghJ5r48++gg9e/ZEUlISbDYbZsyYgb/85S8YMWKE3KURkYx4\n5k3kxbp27Yply5ZBkiTY7XaMHTuWwU1EPPMmIiJSGt6wRkREpDAMbyIiIoVheBMRESmMYm5YKymp\nbn6l3yA8PBDl5VeO36xEbIt38pW2+Eo7ALbFW/lKW9zRDqMxuNH5rfbMW6vVyF1Ci2FbvJOvtMVX\n2gGwLd7KV9riyXa02vAmIiJSKoY3ERGRwjC8iYiIFIbhTUREpDAMbyIiIoVheBMRESkMw5uIiEhh\nGN5EREQK49bwzsjIwKxZs66Yv3PnTkyaNAkpKSn49NNP3VkCERGRz3Hb8KjvvPMOvvrqKwQEBDSY\nb7fbsXLlSmzatAkBAQGYPn06Ro0ahcjISHeVQkRE5FPcFt7R0dF4/fXX8eSTTzaYn5eXh+joaISG\nhgIA+vfvj/3792PcuHHuKoWIiK6TJAk4JQkOp4BTEnA4JTidAg6p7qdTEpAkAQEBIerWFwKQhIAQ\nAlL91xIuzatbJ/h8NSoqrZeW/fI+ABACEKh7DYHLry4tE5dnX7lcuN4FIRq+p/76rh/1ttVg/SY+\nuzGd24eid3QoVCpVM2teP7eF95gxY1BYWHjFfJPJhODgXwZaDwoKgslkanZ74eGBLT5ubFMDvisR\n2+KdfKUtvtIOwHfb4nBKMFvtMNfYYbbaYbE6YLr8usYOa40DtXZn3X+2pn/aHE44HBIczrqgrgtp\nCVJzyUUAgH8+PxahBj+3f47HnypmMBhgNptd02azuUGYN8UdT2pp6SeVyYVt8U6+0hZfaQegvLZI\nQqDKbENpZQ3KqmpQabahymxDtcWGGrtASbmlbtpqg80u/e7PUakAvU4DP60aOq0Gep0GgX5aaDQq\naNRqaDUqaNQqaDXqX35eWqbRXJpWqaBSA2qVCmqVCioVoFZf+qlSQaWq//rSMqigVgHBIf6wmG1Q\nqwCVSgW1WuWqCwBUUKH+yWz9+XC9huuFCg3fX/dahfrnw6r6b7g03dg26n3EFe/5tS6dw2Gz2lBi\ntTX3v/yaNfXHpsfDOy4uDvn5+aioqEBgYCAOHDiAOXPmeLoMIiKvIEkCJZVWnC+14PxFMy6UWVBa\nWYOLVXWB7XA2fcqrUasQEqRH+4ggBPprEeinRcCln4H+WgT4XXrtp4W/nxZ6rRp6nQZ6nRp6rQZ+\nurppjVrlka7epijtj6qmeLIdHgvvzZs3w2KxICUlBYsWLcKcOXMghMCkSZMQFRXlqTKIiGRTa3ei\noMiEU+ercPpCFQqKTbhQZoXDeeVZc0igDp3bGtAmNACRIf6ICPFDmMEPwYE6hATpERvTBpZqq6yh\nS/Jxa3h36tTJ9VWwiRMnuuaPGjUKo0aNcudHExHJrspiQ86ZChzLL8eJwgqcLTU3uBnKT6dBR2MQ\nOrQJRPs2QWjfJgjt2gQiMtQffrqr3+NjCNDBaqpxcwvIW3m825yIyFc5JQnHCypxKLcEx/LLcbbk\nl/t79Fo14jqGoku7ENzQPhg3tAtGVEQg1Dxzpt+B4U1EdB3sDglHTl7EweMlyDhRCnONAwCg06rR\n84ZwdI8OR/eYcNzQLhhaDQe1pJbB8CYi+o2EEDh9oRo/HrmAn49ecAV2eLAfBvaMQnK8EQmdw6DT\nMqzJPRjeRETXyO6QsO9YEb7bX4AzxXXjU4QE6TFmYGcM7BGFG9oF8wYy8giGNxFRM8w1duxMK8TO\ng2dRabZBpQL6JxgxtG979I6NgEbNM2zyLIY3EVETrLUOfHegAN/sK4C11oEAPy3GDozGqP4dERka\n0PwGiNyE4U1E9CtOScLOg2exOfU0TFY7DAE6TB3ZFSMSOyDAj782SX78V0hEVE/OmXJ89N1xFJaY\nEeCnxd3DuuC2AZ0Z2uRV+K+RiAh117U/2Z6LHzMvAACG92uPSSPiEByol7kyoisxvImo1cs8eRHv\n/t8xVJhsiGkXjHv/kIC4DqFyl0XUJIY3EbVadoeEjTtzsfPgWWjUKtw9PBa33xTNu8fJ6zG8iahV\nulhZg7f+nYlT56vQMTIID0zoiZh2vvOsb/JtDG8ianWOnS7D2i+zYLLaMbhXO8we263ZB4EQeROG\nNxG1KqlHzuMfW7MBALPGdMMtiR04KhopDsObiFoFIQQ2fJeDj7ZlI8hfi79M6ouEzmFyl0X0uzC8\nicjnSULg4++OY+fBs4gM9cdjU/qhQ2SQ3GUR/W4MbyLyaZIQ+Oc3Ofg+/RxuaB+C+ZP6INTgJ3dZ\nRNeF4U1EPksSAh9sy8aejPOIbmvA8oeHwGa1yV0W0XXjlxmJyCcJIbBhey72ZJxHTFQwnpiexDNu\n8hkMbyLySf/3cz62pxWiY2QQFkxLhCFAJ3dJRC2G4U1EPueHw+fwr90nERHih79O7cfgJp/D8CYi\nn3L0dBne35qDIH8tHp+aiIgQf7lLImpxDG8i8hklFVa8/WUWVCrg0cl9+XUw8lkMbyLyCbU2J17/\n1xGYrHbc+4cExHfiACzkuxjeRKR4Qgi8t/UYCktMuCWpI0YkdpS7JCK3YngTkeLtTj+HfceK0bVT\nKGbcFi93OURux/AmIkU7W2rGhh25CPLX4uE7ekGr4a818n38V05EimV3OLHuy0zYHBL+dHsP3llO\nrQbDm4gU67NdeSgsMeOWpI5ITjDKXQ6RxzC8iUiRcs6UY3taIdq3CUTKqK5yl0PkUQxvIlIcm92J\n97ZmQ6UC7h/fA346jdwlEXkUw5uIFOff/zmF4nIrRg/ojLgOoXKXQ+RxDG8iUpRT56vwzb4zMIb5\n4+7hsXKXQyQLhjcRKYYkCby/LRtCAH8cx+5yar0Y3kSkGLszzuFMkQlDerdDj5hwucshkg3Dm4gU\nwWS14/PdefDXazDllji5yyGSFcObiBThix9OwlzjwB03d0GowU/ucohkxfAmIq93pqga3x86i3YR\ngbhtQCe5yyGSHcObiLzep7tOQAhgxm3xHLucCAxvIvJyWafLcPR0OXp1iUDv2DZyl0PkFRjeROS1\nJCGw6fs8AMDkEbxJjegyhjcRea0D2cXIv1CNgT3aIqZdsNzlEHkNhjcReSWHU8Lne05Co1ZxJDWi\nX2F4E5FX+jHzAorLrRie2AFR4YFyl0PkVRjeROR1HE4JW348Da1GjQmDb5C7HCKvw/AmIq+z92gR\nSitrMLxfe4QHc0AWol9jeBORV5EkgS0/5UOjVuH2m2LkLofIK7ktvCVJwuLFi5GSkoJZs2YhPz+/\nwfKvvvoKd999NyZNmoSPP/7YXWUQkcLsyy5CUZkFN/dpj4gQf7nLIfJKWndtePv27bDZbNi4cSPS\n09OxatUqrF271rV89erV2LJlCwIDAzF+/HiMHz8eoaGh7iqHiBRAEgJf/5gPtUqF2wfzrJuoKW4L\n77S0NAwbNgwAkJiYiMzMzAbLu3Xrhurqami1WgghoFKp3FUKESlERm4pzpaaMaR3O7QNC5C7HCKv\n5bbwNplMMBgMrmmNRgOHwwGttu4j4+PjMWnSJAQEBGD06NEICQm56vbCwwOh1WpatEaj0XcGfWBb\nvJOvtMVT7dixMR0AMGNcD7d9pq/sE4Bt8UaeaofbwttgMMBsNrumJUlyBXd2dja+//577NixA4GB\ngVi4cCG2bt2KcePGNbm98nJLi9ZnNAajpKS6RbcpF7bFO/lKWzzVjpPnqnD0VBl6x0YgUKNyy2f6\nyj4B2BZv5I52NPXHgNtuWEtOTsaePXsAAOnp6UhISHAtCw4Ohr+/P/z8/KDRaBAREYGqqip3lUJE\nCvDNvjMAgDEDo2WuhMj7ue3Me/To0UhNTcW0adMghMCKFSuwefNmWCwWpKSkICUlBTNmzIBOp0N0\ndDTuvvtud5VCRF6utMKKAznF6GQ0oGdMuNzlEHk9t4W3Wq3G0qVLG8yLi/vlqUDTp0/H9OnT3fXx\nRKQg3x0ohBDAmIGdefMq0TXgIC1EJCtrrQN7Dp9DmEGPQT2j5C6HSBEY3kQkqx8zL6DW5sTI5E7Q\navgrieha8EghItkIIbDr0Flo1CoM79dB7nKIFIPhTUSyOV5QgXOlZgzo3hahQXq5yyFSDIY3Eclm\n58GzAICRSR1lroRIWRjeRCSLSlMtDh4vQUdjEOI78bkGRL8Fw5uIZLEn4xycksCopI78ehjRb8Tw\nJiKPc0oSvk8/Bz+9Bjf1aid3OUSKw/AmIo87fOIiyqtrMaR3OwT4uW2sKCKfxfAmIo/74fB5AMAI\nfj2M6HdheBORR1WaanE47yJiooIRHeUbj4Ek8jSGNxF51E9ZRZCEwNC+7eUuhUixGN5E5DFCCPxw\n+By0GhXHMSe6DgxvIvKYk+eqcP6iBckJRhgCdHKXQ6RYDG8i8pj/HKm7UY1d5kTXh+FNRB5Ra3di\n79EiRIT4oWdMhNzlECkaw5uIPCItpxg1NieG9G4PtZojqhFdD4Y3EXnEj5kXAABD+3BENaLrxfAm\nIrerMNXiWH454jqGoG14oNzlECkew5uI3G7f0SIIAdzUk2fdRC2B4U1Ebvfz0SKoVSrc2L2t3KUQ\n+QSGNxG51YUyC05fqEavLhEICdLLXQ6RT2B4E5Fb/ZxVd6PaTb04ohpRS2F4E5HbCCHwc1YR9Do1\nkuIj5S6HyGcwvInIbU6er0JxhRXJ8Ub46/ncbqKWwvAmIrfZm1UEAHwICVELY3gTkVs4JQn7jhXB\nEKBDry4cDpWoJTG8icgtsvMrUGWx48YebaHV8FcNUUviEUVEbrE/u67LfCC/203U4hjeRNTinJKE\ng8dLERqkR3ynMLnLIfI5DG8ianHZZypgstqR3M3IJ4gRuQHDm4ha3IHsYgDAjd3YZU7kDgxvImpR\nTklCWk4JQoL0SOjMLnMid2B4E1GLyrnUZd4/gV3mRO7C8CaiFnW5y3wA7zInchuGNxG1GEkSSDte\nguBAHbqxy5zIbRjeRNRicgoqUG1hlzmRuzG8iajFsMucyDMY3kTUIi53mRsCdOgWzS5zIndieBNR\ni8gtrECV2Yb+3YzQqPmrhcideIQRUYs4lFsKAEhOMMpcCZHvY3gT0XUTQiA9txR+eg26R4fLXQ6R\nz2N4E9F1O3/RguIKK/p0iYBOy18rRO7Go4yIrlv6ibou88T4SJkrIWodGN5EdN3Sc0uhUgF94xje\nRJ7A8Cai61JltiHvbCXiO4XBEKCTuxyiVkHrrg1LkoQlS5YgJycHer0ey5cvR0xMjGv54cOHsWrV\nKgghYDQa8dJLL8HPz89d5RCRm2TklUIASOzKs24iT3Hbmff27dths9mwceNGLFiwAKtWrXItE0Lg\nueeew8qVK/HJJ59g2LBhOHv2rLtKISI3Ss/l9W4iT3PbmXdaWhqGDRsGAEhMTERmZqZr2alTpxAW\nFoZ//OMfyM3NxYgRIxAbG+uuUojITWx2J7JOl6FdRCDaRQTKXQ5Rq+G28DaZTDAYDK5pjUYDh8MB\nrVaL8vJyHDp0CIsXL0Z0dDQefvhh9O7dG4MHD25ye+HhgdBqNS1ao9EY3KLbkxPb4p18pS1NtWP/\n0Quw2SUM6dtBMW1VSp3Xgm3xPp5qh9vC22AwwGw2u6YlSYJWW/dxYWFhiImJQVxcHABg2LBhyMzM\nvGp4l5dbWrQ+ozEYJSXVLbpNubAt3slX2nK1duxOKwAAJHQMUURbfWWfAGyLN3JHO5r6Y8Bt17yT\nk5OxZ88eAEB6ejoSEhJcyzp37gyz2Yz8/HwAwIEDBxAfH++uUojIDSQhkHGiFIYAHbp2DJW7HKJW\nxW1n3qNHj0ZqaiqmTZsGIQRWrFiBzZs3w2KxICUlBS+88AIWLFgAIQSSkpJwyy23uKsUInKD/AvV\nqDDZcHPvdnx2N5GHXVN45+TkID8/H2q1GtHR0Q3OopuiVquxdOnSBvMud5MDwODBg7Fp06bfWC4R\neQveZU4knybDWwiBTz75BO+//z6CgoLQoUMHaLVaFBYWwmQyYfbs2Zg2bRrUfPQfUauUfqIUWo0K\nvbpEyF0KUavTZHg/+uijGDJkCD799FOEhja8nlVdXY0vvvgCjzzyCNauXev2IonIu5RWWlFQbEKf\n2Dbw17vt6hsRNaHJo+7FF19EYGDj39sMDg7G7NmzMXnyZLcVRkTeK+PERQBAYtc2MldC1Do12ed9\nObgffvhhFBQUNFh23333NViHiFqX9NwSAEA/DolKJItmL1hnZGRgzpw5+OGHH1zzKisr3VoUEXkv\nS40D2WcqEBMVjIgQf7nLIWqVmg3vqKgo/P3vf8dLL72E9evXAwBUKn4thKi1yjx1EU5J8C5zIhk1\nG94qlQqdO3fGxx9/jEOHDmH+/PkQQniiNiLyQuknLn1FjF3mRLJpNrzDwsIA1A13unbtWsTExCA7\nO9vthRGR93FKEo7kXUR4sB+iowzNv4GI3KLJ8K6trQUAvPfeew3mP/74465hTy+vQ0Stw4nCSphr\nHEiMj+TlMyIZNRneTzzxBD799FOYTKYrlgUGBuKjjz7C448/7tbiiMi7HLo0qloSu8yJZNXk97zX\nrFmDTz75BJMnT0ZISAjatWsHjUaDs2fPoqKiArNnz8aaNWs8WSsRyUgIgfTcUvjpNegWHS53OUSt\nWpPhrVarMXPmTMycORPZ2dk4ffq0a2zz7t27e7JGIvIC5y9aUFxhRf9uRui0HBaZSE5Nhvf+/fsb\nTLdpUzeSUnV1Nfbv348bb7zRvZURkVfhXeZE3qPJ8H7ttdcAABUVFSgoKEBSUhLUajUOHTqEhIQE\nbNiwwWNFEpH80nNLoVIBfeM4JCqR3JoM7w8//BAAMHfuXLzxxhuIiYkBAJw9exaLFy/2THVE5BWq\nzDbkna1EfKdQBAfq5S6HqNVr9sLVuXPnXMENAB06dMC5c+fcWhQReZeMvFIIAInxRrlLISJc5cz7\nsl69euGpp57CuHHjIEkStmzZggEDBniiNiLyEumXviLGIVGJvEOz4b18+XL885//dF3jHjJkCGbM\nmOH2wojIO9jsTmSdLkO7iEC0i+CTBIm8QZPhXVJSAqPRiNLSUowdOxZjx451LSsuLkaHDh08UiAR\nyevwiVLY7BLvMifyIk2G97PPPot169bh3nvvhUqlavAwEpVKhR07dnikQCKS196sCwDYZU7kTZoM\n73Xr1gEAdu7c6bFiiMi7SEJgX9YFGAJ0iOsYInc5RHRJs3ebl5WV4bHHHsOgQYMwYMAAzJs3D6Wl\npZ6ojYhkln+hGmVVNegb1wYaNUdVI/IWzR6NixcvRp8+fbBjxw7s3LkT/fr1w3/91395ojYikpnr\nLnNe7ybyKs2Gd0FBAebMmQODwYCQkBDMnTuX3/MmaiXST5RCq1GjV5cIuUshonqaDW+VSoXz58+7\nps+dOwetttlvmBGRwpVWWlFQbELf+EgE+PGYJ/ImzR6R8+fPR0pKCvr16wchBDIyMrBs2TJP1EZE\nMso4cREAMKhXO5krIaJfaza8R44ciX79+uHw4cOQJAnPP/+86wljROS70nNLAAADe7aDsDtkroaI\n6ms2vMvKyvD111+jsrISAHD06FEAwLx589xbGRHJxlrrQPaZCkRHGRAZFoCSkmq5SyKiepq95j13\n7lxXYBNR65B5qgxOSfAucyIvdU13oaxcudLddRCRF7ncZZ7Ep4gReaVmw/u2227DZ599hptuugka\njcY1n2ObE/kmpyThcN5FhAf7ITrKIHc5RNSIZsO7uroa69evR3h4uGsexzYn8l0nCithrnFgYI8o\nqFQqucshokY0G97ffvstfvrpJ/j7+3uiHiKS2SE+u5vI6zV7w1rnzp1dd5oTkW8TQiA9txR+eg26\nR4c3/wYikkWzZ94qlQrjx49HfHw8dDqda/4HH3zg1sKIyPPOX7SguMKK/t2M0Gn5IBIib9VseD/8\n8MOeqIOIvED6CT6IhEgJrmls8/r/qdVqBAQEoKqqyhP1EZEHpeeWQqUC+sRxFEUib9bsmfebb76J\nzMxMDB48GEII7Nu3Dx07doTJZML8+fMxYcIET9RJRG5WZbYh72wlunYKRUigXu5yiOgqmg1vIQS+\n+uor1/e6i4qK8Mwzz+DDDz/ErFmzGN5EPiLjRCkEeJc5kRI0221eXFzcYECWqKgoFBcXw2AwQAjh\n1uKIyHMuX+/mqGpE3q/ZM++kpCQsWLAAEydOhCRJ+Prrr5GUlITvv/8egYGBnqiRiNys1u5E1qky\ntG8TiHYRPK6JvF2z4b106VJs2LABGzduhEajwZAhQzB16lSkpqZi9erVnqiRiNzs6Oky2BwSu8yJ\nFKLJ8C4pKYHRaERxcTFGjRqFUaNGuZYVFxdjxIgRHimQiNzv8qhq7DInUoYmw/vZZ5/FunXrcO+9\n9zY6vjHHNifyDZIkkHGiFCFBesR2CJG7HCK6Bk3esLZu3ToAwN/+9jfMnDkTW7duRUxMDEwmExYu\nXOixAonIvfLOVaLaYkdi1zZQ80EkRIrQ7N3mL7zwAvr06YNvv/0W/v7++Pe//4133nnHE7URkQf8\n8iASdpkTKUWz4S1JEm688Ubs2rULf/jDH9C+fXs4nc5mNyxJEhYvXoyUlBTMmjUL+fn5ja733HPP\n4eWXX/7tlRNRiziUWwq9To2eMXwQCZFSNBveAQEBePfdd7F3716MHDkS77//PoKCgprd8Pbt22Gz\n2bBx40YsWLAAq1atumKdDRs24Pjx47+vciK6bucvmlFUZkGvGyKg12nkLoeIrlGz4f3yyy/DYrHg\ntddeQ2hoKIqLi/HKK680u+G0tDQMGzYMAJCYmIjMzMwGyw8ePIiMjAykpKT8ztKJ6Hql8y5zIkVq\n9nveUVFRmDdvnmv6Wm9WM5lMMBgMrmmNRgOHwwGtVovi4mK8+eabeOONN7B169Zr2l54eCC02pY9\nMzAag1t0e3JiW7yTt7cl83Q51Cpg1KAYhBr8mlzP29vxW7At3slX2uKpdjQb3r+XwWCA2Wx2TUuS\nBK227uOnxergAAAYsElEQVS2bduG8vJyPPjggygpKUFNTQ1iY2Nxzz33NLm98nJLi9ZnNAajpKS6\nRbcpF7bFO3l7WyrNNmSfLkN8p1DYrDaUWG2Nruft7fgt2Bbv5CttcUc7mvpjwG3hnZycjF27duH2\n229Heno6EhISXMtmz56N2bNnAwA+//xznDx58qrBTUQt75cHkbDLnEhp3Bbeo0ePRmpqKqZNmwYh\nBFasWIHNmzfDYrHwOjeRF3Bd707gkKhESuO28Far1Vi6dGmDeXFxcVesxzNuIs+rtTmRdboMHSKD\nEBXOB5EQKU2zd5sTke85cvIi7A4JyTzrJlIkhjdRK3QgpxgA0D+hrcyVENHvwfAmamXsDicy8i4i\nMtQf0VGG5t9ARF6H4U3UymSdLketzYkB3do2+sRAIvJ+DG+iVibtcpd5N35FjEipGN5ErYjDKSE9\ntxThwX7owmd3EykWw5uoFck5UwFzjQPJ8UY+u5tIwRjeRK0Iu8yJfAPDm6iVkCSBg8dLEByoQ0Ln\nMLnLIaLrwPAmaiVyCytQZbEjKd4ItZpd5kRKxvAmaiXSckoAAAPYZU6keAxvolZACIG04yUI9NOi\ne0y43OUQ0XVieBO1AnnnqlBeXYvE+EhoNTzsiZSORzFRK7DvaBEAYGCPKJkrIaKWwPAm8nGSJLA/\nuxhB/lr0vIFd5kS+gOFN5ONyCipQabZhQPe27DIn8hE8kol83L5j7DIn8jUMbyIf5nBKOJBdjNAg\nPbpxYBYin8HwJvJhR0+Xw1zjwI3d23JgFiIfwvAm8mGuLvOe7DIn8iUMbyIfZXc4cfB4CdqE+COO\nj/8k8ikMbyIfdTivDDU2Jwb2aAsVH/9J5FMY3kQ+ai/vMifyWQxvIh9kqbEjPbcU7dsEIjrKIHc5\nRNTCGN5EPmh/djEcTglDerdjlzmRD2J4E/mg1MwLUAEY3Kud3KUQkRswvIl8THG5BScKK9E9JhwR\nIf5yl0NEbsDwJvIxP2ZeAADc3Idn3US+iuFN5EMkIfBj5gX46TRITjDKXQ4RuQnDm8iHnCisRGll\nDfp3M8Jfr5W7HCJyE4Y3kQ9JPXIeADCkN7vMiXwZw5vIR9TanDiQU4yIED90jwmXuxwiciOGN5GP\n2J9dDGutE0N6t4ea3+0m8mkMbyIfsTvjLFQAhvdtL3cpRORmDG8iH1BYYkLe2Sr06hKByLAAucsh\nIjdjeBP5gD3p5wAAIxI7yFwJEXkCw5tI4Wx2J37KuoCQID36dY2Uuxwi8gCGN5HCpeWUwFzjwNA+\n7aHV8JAmag14pBMp3O6Mui7z4f14oxpRa8HwJlKw8xfNOF5QgR4x4WgbHih3OUTkIQxvIgXbkVYI\nABiZ1FHmSojIkxjeRAplqXEg9cgFhAf7ISmBN6oRtSYMbyKF+s+R86i1OzEquSM0ah7KRK0Jj3gi\nBZIkgR1pBdBp1Rjej9/tJmptGN5ECnT45EWUVNTgpp5RCA7Uy10OEXkYw5tIgS7fqHZr/04yV0JE\ncmB4EylMYYkJWafKkNA5DNFRwXKXQ0Qy0Lprw5IkYcmSJcjJyYFer8fy5csRExPjWr5lyxa8//77\n0Gg0SEhIwJIlS6DmTTdEzdr68xkAwNiB0TJXQkRycVtabt++HTabDRs3bsSCBQuwatUq17Kamhr8\nz//8Dz744ANs2LABJpMJu3btclcpRD7jYmUN9h0rQofIIPTt2kbucohIJm4L77S0NAwbNgwAkJiY\niMzMTNcyvV6PDRs2ICCg7tGFDocDfn5+7iqFyGd8s/8MnJLAuEHRUKtUcpdDRDJxW7e5yWSCwWBw\nTWs0GjgcDmi1WqjVakRG1g0q8eGHH8JiseDmm2++6vbCwwOh1WpatEaj0XeuF7It3qkl21JltuGH\nw+cRGeqP8cO7Qqf13GUm7hPvxLZ4H0+1w23hbTAYYDabXdOSJEGr1TaYfumll3Dq1Cm8/vrrUDVz\nFlFebmnR+ozGYJSUVLfoNuXCtninlm7LV/85hVqbE3cP7YKKcnPzb2gh3CfeiW3xPu5oR1N/DLjt\nT/fk5GTs2bMHAJCeno6EhIQGyxcvXoza2lq89dZbru5zImqctdaB7WmFCPLXYngiB2Uhau3cduY9\nevRopKamYtq0aRBCYMWKFdi8eTMsFgt69+6NTZs2YcCAAbjvvvsAALNnz8bo0aPdVQ6Rou08WAiT\n1Y67hnaBv95thy0RKYTbfguo1WosXbq0wby4uDjX6+zsbHd9NJFPsdY6sG3vGQT5azH6xs5yl0NE\nXoBfrCbyct8dKIC5xoExA6MR4MezbiJieBN5NUuNHd/sK4AhQMehUInIheFN5MW+3V8Aa60D4wbx\nrJuIfsHwJvJSlaZafLOvACGBOoxK5lk3Ef2C4U3kpb744RRq7U7cOSwWfvqWHaCIiJSN4U3khQpL\nTPjh8Dl0iAzC8H7t5S6HiLwMw5vIC322Kw9CAFNHxkHDp+0R0a/wtwKRl8k6VYYjJy+iR0w4+sTy\nyWFEdCWGN5EXcTglfLz9OFQAUkZ1bXbMfyJqnRjeRF7km31ncP6iBSOTOyI6yjeeskRELY/hTeQl\nSiqs2Jx6GiFBetwzPFbucojIizG8ibyAEAIffXccNoeEaaO6ItBfJ3dJROTFGN5EXiAtpwSH8+pu\nUhvUM0rucojIyzG8iWRWZbHhw29zoNWoce8fEniTGhE1i+FNJCMhBD7cloNqix2TR8SifZsguUsi\nIgVgeBPJ6OejRUg7XoKEzmG4jc/qJqJrxPAmkklZVQ0++vY4/HQa3D++B9TsLieia8TwJpKBwynh\n7a+yYKl1IOXWrmgbFiB3SUSkIAxvIhl8vuckThRWYmCPthjRr4Pc5RCRwjC8iTzsUG4Jtu09g6iI\nQNw3tjvvLiei34zhTeRBReUW/H3LMei0avy/u3ojwE8rd0lEpEAMbyIPMdfY8T+fHYal1oF7/5CA\nzm0NcpdERArF8CbyAIdTwltfZKKozIKxg6IxrC+vcxPR78fwJnIzIQT++W0OjuWXIyk+EpNviZO7\nJCJSOIY3kRsJIbBpdx72ZJxHdJQBD07sxe9zE9F1Y3gTudGWn/Kx9ee6O8v/OjURfnqN3CURkQ9g\neBO5yZd78vDFnpNoE+KPhdMSERqkl7skIvIR/J4KUQsTQuDrn/Lx+Z6TCA3S44npiYgI8Ze7LCLy\nIQxvohYkhMBn3+dh294zMIYH4PEp/RAVHih3WUTkYxjeRC3E7pDwwTfZSD1yAe0iArHi/w0FHA65\nyyIiH8TwJmoBVRYb3vz8CHILK3FDu2A8NrUfjOEBKCmplrs0IvJBDG+i63SmqBqv/+sILlbVYGCP\ntvjT7T3gp+Nd5UTkPgxvot9JCIGdB89i484TcDgl3DW0CybefAMfNEJEbsfwJvodqi02vL8tBweP\nl8AQoMMDE3qjb1yk3GURUSvB8Cb6DYQQ2Hu0CB9vz4XJakf36DDMndgL4cF+cpdGRK0Iw5voGpVW\nWPHP747jcN5F6LVqpIzqitEDOkOtZjc5EXkWw5uoGZYaO77+KR/fHSiEwymhR0w47hvXHW3DAuQu\njYhaKYY3URNq7U7sTj+HLT+ehslqR0SIHyYNj8NNvaJ4UxoRyYrhTfQr1loHdh4sxLf7C1BtscNf\nr8GkEbEYPaAz9PwKGBF5AYY30SVnS834/uBZ/Jh1HtZaJwL8tJgw5AaMHtAJwYF8qAgReQ+GN7Vq\n1loHDuWWYE/GeRwvqAAAhBr0GDcoBqOSOyHQn4cIEXkf/maiVqfG5kDmyTLsO1aEjLyLsDskAEDP\nG8IxMqkj+nWNhFbDp+USkfdieJPPE0LgQpkFR06W4UheKXIKKuBwCgBA+zaBGNQjCoN6RiEqgk//\nIiJlYHiTz5EkgcISE3ILK3G8oAK5hRWoMNlcy6OjDOgb1wYDurVF57YG3jlORIrD8CZFq7U7ca7U\njDNF1ThTbMKZomoUFptRa3e61gkJ0mNANyN6x7ZBn9g2HA2NiBSP4U1eTQgBc40D5dW1uFhZg6Jy\nC4rKLCgqt+JCmQXl1bUN1lerVOgQGYgb2ocgoVMY4juHom1YAM+uicinuC28JUnCkiVLkJOTA71e\nj+XLlyMmJsa1fOfOnXjzzTeh1WoxadIkTJ061V2lkJeRhIClxgGT1Q6TxQ6T1Y5qqw0mqx0SVDhX\nbEJ5dQ3Kq2tRXl0L26Ubyn4tIsQPPWLC0b5NIKKjghEdZUDHyCDotPwuNhH5NreF9/bt22Gz2bBx\n40akp6dj1apVWLt2LQDAbrdj5cqV2LRpEwICAjB9+nSMGjUKkZF8KpOnCSEgCQFJqrtWLF2adjgF\nHA4Jdqfk+ml3SHA46/6zN5hXt26t3YkamxO1NidqbA7UXP55aX5Nbd20pdYBIZqvLSRQh/ZtghAe\n7IfwED9EBPshKjwQ7SICYQwP4DOziajVclt4p6WlYdiwYQCAxMREZGZmupbl5eUhOjoaoaGhAID+\n/ftj//79GDdunLvKaaDSVIuPd5xAeZW1boYALmeJqJcql19enlc/b4QARL03inpvEPXe29g2Rb0J\n1+e66vhlg6LeewDxSz31ihEQ0Go0sNkdkMSlAK4Xwk5JQEgCkgCcl+YLSfzy+hpC9Hpp1Cr46zXw\n12sQavBDh8ggGAJ0CA7UIShAh+AAPYICtAgO0KNT+1AIhwNhBj/otPy6FhFRY9wW3iaTCQaDwTWt\n0WjgcDig1WphMpkQHBzsWhYUFASTyXTV7YWHB0LbQt2hxdU27EwrgCR5ILmaoVIBqnoTqkvzLs1w\nvVZdXl5vuv56arUKapUKGk3dT7VaBZ1GDb9G5mvUja9ff75Wo4ZOp4Zeq4FOq673X920/lfTOq0a\nfnoNAvy0V/yn06pb7TVnozG4+ZUUwFfaAbAt3spX2uKpdrgtvA0GA8xms2takiRotdpGl5nN5gZh\n3pjyckuL1dY2WI9Plo3D+aIqV3DWD5cGgVovQH9Z9kuCNhW2qDdfdWnFxj6rJRiNwSgpqW7RbV43\nSYLNaoPNamt+3Xq8si2/k6+0xVfaAbAt3spX2uKOdjT1x4Dbwjs5ORm7du3C7bffjvT0dCQkJLiW\nxcXFIT8/HxUVFQgMDMSBAwcwZ84cd5XSqEB/HUI4XjURESmQ28J79OjRSE1NxbRp0yCEwIoVK7B5\n82ZYLBakpKRg0aJFmDNnDoQQmDRpEqKiotxVChERkU9xW3ir1WosXbq0wby4uDjX61GjRmHUqFHu\n+ngiIiKfxdt5iYiIFIbhTUREpDAMbyIiIoVheBMRESkMw5uIiEhhGN5EREQKw/AmIiJSGIY3ERGR\nwjC8iYiIFEYlhCceCklEREQthWfeRERECsPwJiIiUhiGNxERkcIwvImIiBSG4U1ERKQwDG8iIiKF\n0cpdgCd899132LZtG1555RUAQHp6Ol544QVoNBoMHToU8+bNa7B+TU0NFi5ciIsXLyIoKAgvvvgi\nIiIi5Ci9UevXr8cPP/wAAKiqqkJpaSlSU1MbrLN8+XIcPHgQQUFBAIC33noLwcHBHq+1OUIIDB8+\nHDfccAMAIDExEQsWLGiwzqeffooNGzZAq9Xiz3/+M0aOHClDpVdXXV2NhQsXwmQywW63Y9GiRUhK\nSmqwjrfvE0mSsGTJEuTk5ECv12P58uWIiYlxLd+5cyfefPNNaLVaTJo0CVOnTpWx2qbZ7XY888wz\nOHv2LGw2G/785z/j1ltvdS3/xz/+gc8++8x1TD///POIjY2Vq9xm3X333TAYDACATp06YeXKla5l\nStknAPD555/jiy++AADU1tbi2LFjSE1NRUhICADl7JeMjAy8/PLL+PDDD5Gfn49FixZBpVIhPj4e\n//3f/w21+pdz4uaOqesifNyyZcvEmDFjxGOPPeaad8cdd4j8/HwhSZJ44IEHRFZWVoP3vPvuu+K1\n114TQgixZcsWsWzZMo/W/Fs8+OCD4ocffrhi/rRp08TFixdlqOi3OX36tHjooYeaXF5cXCwmTJgg\namtrRVVVleu1t1mzZo147733hBBC5OXlibvuuuuKdbx9n3zzzTfiqaeeEkIIcejQIfHwww+7ltls\nNnHbbbeJiooKUVtbK+655x5RUlIiV6lXtWnTJrF8+XIhhBDl5eVixIgRDZYvWLBAHDlyRIbKfrua\nmhpx5513NrpMSfvk15YsWSI2bNjQYJ4S9sv69evFhAkTxJQpU4QQQjz00EPi559/FkII8dxzz4lv\nv/22wfpXO6aul893mycnJ2PJkiWuaZPJBJvNhujoaKhUKgwdOhQ//vhjg/ekpaVh2LBhAIDhw4fj\np59+8mTJ1+zbb79FSEgIhg4d2mC+JEnIz8/H4sWLMW3aNGzatEmmCpuXlZWFoqIizJo1C3PnzsXJ\nkycbLD98+DCSkpKg1+sRHByM6OhoZGdny1Rt0/74xz9i2rRpAACn0wk/P78Gy5WwT+r/u09MTERm\nZqZrWV5eHqKjoxEaGgq9Xo/+/ftj//79cpV6VWPHjsX8+fMB1PXsaDSaBsuzsrKwfv16TJ8+HevW\nrZOjxGuWnZ0Nq9WK+++/H7Nnz0Z6erprmZL2SX1HjhzBiRMnkJKS0mC+EvZLdHQ0Xn/9ddd0VlYW\nBg4cCKAuK66WJb8+pq6Xz3Sbf/bZZ3j//fcbzFuxYgVuv/127N271zXPZDK5uqAAICgoCAUFBQ3e\nZzKZXN2ZQUFBqK6udmPlV9dUu/r27Yt169bh1VdfveI9FosF9957L/70pz/B6XRi9uzZ6N27N7p3\n7+6pshvVWFsWL16MBx98EOPGjcOBAwewcOFC/Otf/3Itr78vgLr9YTKZPFZzY662T0pKSrBw4UI8\n88wzDZZ76z6p79fHhkajgcPhgFar9cr90JTLlyVMJhMeffRRPPbYYw2Wjx8/HjNmzIDBYMC8efOw\na9cur7wUAwD+/v6YM2cOpkyZgtOnT2Pu3LnYtm2b4vZJfevWrcMjjzxyxXwl7JcxY8agsLDQNS2E\ngEqlAtB4VlztmLpePhPeU6ZMwZQpU5pdz2AwwGw2u6bNZrPrmktj6zS23JOaateJEycQEhLS6PWT\ngIAAzJ49GwEBAQCAm266CdnZ2bIHRWNtsVqtrjOjAQMGoLi4uMEB0dj+kvs6cVP7JCcnB48//jie\nfPJJ11/jl3nrPqnv1/+vJUly/ZLxxv1wNefPn8cjjzyCGTNmYOLEia75Qgjcd999rtpHjBiBo0eP\nel1IXNalSxfExMRApVKhS5cuCAsLQ0lJCdq3b6+4fQLU3aNz6tQp3HTTTQ3mK22/XFb/+nZzWQI0\nPKau+7NbZCsKYjAYoNPpcObMGQgh8J///AcDBgxosE5ycjJ2794NANizZw/69+8vR6lX9eOPP2L4\n8OGNLjt9+jSmT58Op9MJu92OgwcPolevXh6u8Nq88cYbrrPY7OxstG/f3hXcANC3b1+kpaWhtrYW\n1dXVyMvLQ0JCglzlNunEiROYP38+XnnlFYwYMeKK5UrYJ8nJydizZw+Aups66/9/jouLQ35+Pioq\nKmCz2XDgwIErbsjzFqWlpbj//vuxcOFCTJ48ucEyk8mECRMmwGw2QwiBvXv3onfv3jJV2rxNmzZh\n1apVAICioiKYTCYYjUYAytonl+3fvx+DBw++Yr7S9stlPXv2dPXs7tmzp9EsaeqYul4+c+b9Wzz/\n/PN44okn4HQ6MXToUPTr1w8AcP/99+Ptt9/G9OnT8dRTT2H69OnQ6XSuu9S9yalTp3DzzTc3mPfe\ne+8hOjoat956K+68805MnToVOp0Od955J+Lj42Wq9OoefPBBLFy4ELt374ZGo3HdSVu/LbNmzcKM\nGTMghMBf//rXK64ne4NXXnkFNpsNL7zwAoC6PxLXrl2rqH0yevRopKamYtq0aRBCYMWKFdi8eTMs\nFgtSUlKwaNEizJkzB0IITJo0CVFRUXKX3Ki3334bVVVVeOutt/DWW28BqOstsVqtSElJwV//+lfM\nnj0ber0egwcPbvSPLW8xefJkPP3005g+fTpUKhVWrFiBrVu3Km6fXHbq1Cl06tTJNV3/35eS9stl\nTz31FJ577jm8+uqriI2NxZgxYwAATz75JB577LFGj6mWwqeKERERKUyr6zYnIiJSOoY3ERGRwjC8\niYiIFIbhTUREpDAMbyIiIoVheBMRESkMw5uIiEhhWuUgLUTUvA8++MA1znxNTQ0KCgqwe/du1whf\nRCQfDtJCRFclhMC8efOQlJSEBx54QO5yiAjsNieiZqxZswZ6vZ7BTeRF2G1ORE3aunUrdu3ahQ0b\nNshdChHVw/AmokYdO3YMq1evxgcffOB6lCkReQde8yaiRt1///3Izc2F0WiE0+kEADz33HNXPPaQ\niDyP4U1ERKQwvGGNiIhIYRjeRERECsPwJiIiUhiGNxERkcIwvImIiBSG4U1ERKQwDG8iIiKFYXgT\nEREpzP8HUr63NSsl5A8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba9c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
